{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b41cb40",
   "metadata": {},
   "source": [
    "# NeMo Curator: Complete Guide to Preprocessing, Embeddings, and Vector Storage\n",
    "\n",
    "This comprehensive tutorial will guide you through the complete pipeline of preprocessing data, generating embeddings, and storing vectors for retrieval applications using NVIDIA NeMo Curator.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. **Environment Setup** - Configure NeMo Curator with GPU acceleration\n",
    "2. **Data Loading & Preprocessing** - Clean and prepare raw text data\n",
    "3. **Document Chunking** - Optimize text segmentation for embeddings\n",
    "4. **Embedding Generation** - Create high-quality text embeddings at scale\n",
    "5. **Vector Storage** - Set up efficient vector databases for retrieval\n",
    "6. **Similarity Search** - Implement and test retrieval systems\n",
    "7. **Performance Optimization** - Scale and optimize your pipeline\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "- NVIDIA GPU with CUDA support\n",
    "- Python 3.8+ \n",
    "- NeMo Curator installed with GPU acceleration\n",
    "- Basic familiarity with text processing concepts\n",
    "\n",
    "Let's get started building your preprocessing and embedding pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fe991",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and configure all necessary libraries for our preprocessing and embedding pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76383c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up imports from source...\n",
      "‚úÖ Core NeMo Curator imports successful!\n",
      "‚ö†Ô∏è  Semantic dedup modules not available: cannot import name 'SemDedup' from 'nemo_curator.modules.semantic_dedup' (/Users/productpat/Curator/nemo_curator/modules/semantic_dedup/__init__.py)\n",
      "‚úÖ Text processing modules imported!\n",
      "‚úÖ Progress tracking available!\n",
      "‚úÖ All imports configured!\n",
      "‚ö†Ô∏è  CUDA not available, using CPU\n",
      "‚ö†Ô∏è  GPU acceleration not available, using CPU backend\n"
     ]
    }
   ],
   "source": [
    "# Setup for NeMo Curator from source\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the Curator directory to Python path since we're running from source\n",
    "curator_path = \"/Users/productpat/Curator\"\n",
    "if curator_path not in sys.path:\n",
    "    sys.path.insert(0, curator_path)\n",
    "\n",
    "print(\"üîß Setting up imports from source...\")\n",
    "\n",
    "try:\n",
    "    # Core NeMo Curator imports\n",
    "    from nemo_curator.datasets import DocumentDataset\n",
    "    from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "    from nemo_curator.modules import (\n",
    "        AddId, \n",
    "        ExactDuplicates, \n",
    "        FuzzyDuplicates,\n",
    "        Sequential\n",
    "    )\n",
    "    print(\"‚úÖ Core NeMo Curator imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import core NeMo Curator: {e}\")\n",
    "\n",
    "try:\n",
    "    from nemo_curator.modules.semantic_dedup import (\n",
    "        SemDedup,\n",
    "        EmbeddingCreator,\n",
    "    )\n",
    "    print(\"‚úÖ Semantic deduplication modules imported!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Semantic dedup modules not available: {e}\")\n",
    "\n",
    "try:\n",
    "    # Text processing imports\n",
    "    from nemo_curator.modifiers import UnicodeReformatter\n",
    "    from nemo_curator.filters import FastTextLangId\n",
    "    from nemo_curator import Modify, ScoreFilter\n",
    "    print(\"‚úÖ Text processing modules imported!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Text processing modules not available: {e}\")\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"‚úÖ Progress tracking available!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  tqdm not available, progress bars disabled\")\n",
    "    tqdm = lambda x: x  # Simple fallback\n",
    "\n",
    "print(\"‚úÖ All imports configured!\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ CUDA available: {torch.cuda.device_count()} GPU(s)\")\n",
    "        print(f\"   Current device: {torch.cuda.get_device_name()}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  CUDA not available, using CPU\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  PyTorch not available\")\n",
    "\n",
    "# Check if cuDF is available for GPU acceleration\n",
    "try:\n",
    "    import cudf\n",
    "    print(\"‚úÖ GPU acceleration (cuDF) available\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  GPU acceleration not available, using CPU backend\")\n",
    "    USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3944ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPU-accelerated Dask client for distributed processing\n",
    "print(\"üöÄ Initializing distributed computing environment...\")\n",
    "\n",
    "try:\n",
    "    # Start GPU cluster for semantic deduplication and embeddings\n",
    "    client = get_client(cluster_type=\"gpu\", set_torch_to_use_rmm=False)\n",
    "    print(f\"‚úÖ GPU cluster initialized with {get_num_workers(client)} workers\")\n",
    "    print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GPU cluster failed, falling back to CPU: {e}\")\n",
    "    # Fallback to CPU cluster\n",
    "    client = get_client(cluster_type=\"cpu\", n_workers=4, processes=True, memory_limit=\"8GB\")\n",
    "    print(f\"‚úÖ CPU cluster initialized with {get_num_workers(client)} workers\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import cudf\n",
    "    print(\"‚úÖ GPU acceleration (cuDF) available\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  GPU acceleration not available, using CPU backend\")\n",
    "    USE_GPU = False\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063aa15",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Preprocessing\n",
    "\n",
    "Let's start by loading and validating our raw data. NeMo Curator supports various input formats including JSONL, Parquet, and plain text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data directory and files for this tutorial\n",
    "data_dir = Path(\"tutorial_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample documents for our tutorial\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"text\": \"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and human language. It involves developing algorithms and models that can understand, interpret, and generate human language.\",\n",
    "        \"source\": \"AI_encyclopedia\",\n",
    "        \"category\": \"technology\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning is a method of data analysis that automates analytical model building. It uses algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look.\",\n",
    "        \"source\": \"ML_handbook\", \n",
    "        \"category\": \"technology\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Climate change refers to long-term shifts in global or regional climate patterns. The primary cause is increased levels of greenhouse gases produced by human activities, particularly the burning of fossil fuels.\",\n",
    "        \"source\": \"climate_report\",\n",
    "        \"category\": \"environment\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Quantum computing leverages quantum mechanical phenomena like superposition and entanglement to process information in fundamentally different ways than classical computers. This enables solving certain complex problems exponentially faster.\",\n",
    "        \"source\": \"quantum_physics\",\n",
    "        \"category\": \"technology\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Biodiversity refers to the variety of life on Earth, including the variety of species, ecosystems, and genetic diversity within species. It is essential for ecosystem stability and human well-being.\",\n",
    "        \"source\": \"biology_textbook\",\n",
    "        \"category\": \"environment\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save sample data as JSONL\n",
    "sample_file = data_dir / \"sample_documents.jsonl\"\n",
    "with open(sample_file, 'w') as f:\n",
    "    for doc in sample_documents:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created sample data: {sample_file}\")\n",
    "print(f\"üìÑ Number of documents: {len(sample_documents)}\")\n",
    "\n",
    "# Display first document\n",
    "print(f\"üìã Sample document:\")\n",
    "print(json.dumps(sample_documents[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using NeMo Curator's DocumentDataset\n",
    "print(\"üìö Loading data with NeMo Curator...\")\n",
    "\n",
    "# Method 1: Load from JSONL files\n",
    "backend = \"cudf\" if USE_GPU else \"pandas\"\n",
    "dataset = DocumentDataset.read_json(str(data_dir), backend=backend)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Dataset info:\")\n",
    "print(f\"   - Number of partitions: {dataset.df.npartitions}\")\n",
    "print(f\"   - Backend: {backend}\")\n",
    "print(f\"   - Columns: {list(dataset.df.columns)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "df_sample = dataset.df.head()\n",
    "print(f\"\\nüìã First few documents:\")\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c691b41",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning and Normalization\n",
    "\n",
    "Before generating embeddings, we need to clean and normalize our text data. This includes Unicode normalization, removing unwanted characters, and filtering by language quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique document IDs first\n",
    "print(\"üè∑Ô∏è  Adding unique document IDs...\")\n",
    "add_id = AddId(id_field=\"doc_id\", id_prefix=\"doc\", start_index=0)\n",
    "dataset_with_ids = add_id(dataset)\n",
    "\n",
    "print(f\"‚úÖ Added IDs. Sample with ID:\")\n",
    "print(dataset_with_ids.df.head(2).compute())\n",
    "\n",
    "# Unicode normalization to handle various text encodings\n",
    "print(\"\\nüßπ Applying Unicode normalization...\")\n",
    "unicode_normalizer = UnicodeReformatter()\n",
    "normalize_step = Modify(unicode_normalizer)\n",
    "normalized_dataset = normalize_step(dataset_with_ids)\n",
    "\n",
    "print(\"‚úÖ Unicode normalization complete\")\n",
    "\n",
    "# Check for text improvements\n",
    "original_sample = dataset_with_ids.df.head(1)['text'].compute().iloc[0]\n",
    "normalized_sample = normalized_dataset.df.head(1)['text'].compute().iloc[0]\n",
    "\n",
    "print(f\"\\nüìù Text normalization example:\")\n",
    "print(f\"Original: {original_sample[:100]}...\")\n",
    "print(f\"Normalized: {normalized_sample[:100]}...\")\n",
    "\n",
    "print(f\"\\nüìä Dataset size after normalization: {len(normalized_dataset.df)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c4494",
   "metadata": {},
   "source": [
    "## 4. Document Chunking and Segmentation\n",
    "\n",
    "For effective embedding generation, we need to split long documents into smaller, semantically coherent chunks. This is crucial for retrieval applications where you want to find specific relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import document splitting modules\n",
    "from nemo_curator.modules import DocumentSplitter, DocumentJoiner\n",
    "\n",
    "print(\"‚úÇÔ∏è  Setting up document chunking pipeline...\")\n",
    "\n",
    "# For this example, let's create some longer documents to demonstrate chunking\n",
    "longer_docs = [\n",
    "    {\n",
    "        \"text\": \"Natural language processing (NLP) is a subfield of artificial intelligence. It focuses on interaction between computers and human language. NLP involves developing algorithms that can understand text. These algorithms can interpret human language patterns. They can also generate human-like text responses. Modern NLP uses deep learning techniques. Transformers have revolutionized the field. Applications include translation, summarization, and chatbots.\",\n",
    "        \"doc_id\": \"doc_long_1\",\n",
    "        \"source\": \"AI_encyclopedia\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning automates analytical model building. It uses algorithms that learn from data iteratively. Computers find hidden insights automatically. No explicit programming is needed for pattern discovery. Supervised learning uses labeled training data. Unsupervised learning finds patterns in unlabeled data. Reinforcement learning learns through trial and error. Deep learning uses neural networks with multiple layers.\",\n",
    "        \"doc_id\": \"doc_long_2\", \n",
    "        \"source\": \"ML_handbook\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a new dataset with longer documents for chunking demonstration\n",
    "longer_df = pd.DataFrame(longer_docs)\n",
    "chunking_dataset = DocumentDataset.from_pandas(longer_df, backend=backend)\n",
    "\n",
    "print(f\"üìÑ Created dataset with {len(longer_docs)} longer documents for chunking\")\n",
    "print(f\"üìù Sample document length: {len(longer_docs[0]['text'])} characters\")\n",
    "\n",
    "# Split documents by sentences (using period as separator)\n",
    "print(\"\\nüî™ Splitting documents into smaller chunks...\")\n",
    "splitter = DocumentSplitter(\n",
    "    separator=\".\", \n",
    "    text_field=\"text\",\n",
    "    segment_id_field=\"segment_id\"\n",
    ")\n",
    "\n",
    "chunked_dataset = splitter(chunking_dataset)\n",
    "print(f\"‚úÖ Document splitting complete\")\n",
    "\n",
    "# Show the results\n",
    "chunked_sample = chunked_dataset.df.compute()\n",
    "print(f\"\\nüìä Chunking results:\")\n",
    "print(f\"   - Original documents: {len(longer_docs)}\")\n",
    "print(f\"   - Total chunks created: {len(chunked_sample)}\")\n",
    "print(f\"\\nüìã Sample chunks:\")\n",
    "print(chunked_sample[['doc_id', 'segment_id', 'text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94187c9f",
   "metadata": {},
   "source": [
    "## 5. Embedding Model Selection and Loading\n",
    "\n",
    "Now we'll configure and load an embedding model to convert our text chunks into dense vector representations. We'll use sentence-transformers which integrates well with NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure embedding model settings\n",
    "print(\"ü§ñ Configuring embedding model...\")\n",
    "\n",
    "# Model configuration - using a lightweight, high-quality sentence transformer\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBEDDING_BATCH_SIZE = 128  # Adjust based on your GPU memory\n",
    "EMBEDDING_DIM = 384  # Dimension of the all-MiniLM-L6-v2 model\n",
    "\n",
    "print(f\"üìã Embedding Configuration:\")\n",
    "print(f\"   - Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"   - Batch size: {EMBEDDING_BATCH_SIZE}\")\n",
    "print(f\"   - Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"   - GPU acceleration: {USE_GPU}\")\n",
    "\n",
    "# Test embedding model loading\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print(f\"\\nüîÑ Loading embedding model...\")\n",
    "    test_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    \n",
    "    # Test with a sample text\n",
    "    test_text = \"This is a test sentence for embedding generation.\"\n",
    "    test_embedding = test_model.encode([test_text])\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìä Test embedding shape: {test_embedding.shape}\")\n",
    "    print(f\"üî¢ Sample embedding values: {test_embedding[0][:5]}...\")\n",
    "    \n",
    "    # Clean up test model\n",
    "    del test_model\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading embedding model: {e}\")\n",
    "    print(\"Please ensure sentence-transformers is installed: pip install sentence-transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47e434",
   "metadata": {},
   "source": [
    "## 6. Batch Embedding Generation\n",
    "\n",
    "Now we'll use NeMo Curator's EmbeddingCreator to generate embeddings for all our text chunks at scale. This module handles distributed processing, GPU acceleration, and memory management automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c529696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding generation using NeMo Curator's EmbeddingCreator\n",
    "print(\"üöÄ Starting distributed embedding generation...\")\n",
    "\n",
    "# Create output directory for embeddings\n",
    "embeddings_dir = data_dir / \"embeddings\"\n",
    "embeddings_dir.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Initialize the EmbeddingCreator\n",
    "    embedding_creator = EmbeddingCreator(\n",
    "        model_name_or_path=EMBEDDING_MODEL,\n",
    "        text_field=\"text\",\n",
    "        embedding_field=\"embeddings\",\n",
    "        batch_size=EMBEDDING_BATCH_SIZE,\n",
    "        embedding_save_loc=str(embeddings_dir),\n",
    "        write_embeddings_to_disk=True,\n",
    "        id_field=\"doc_id\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ EmbeddingCreator initialized\")\n",
    "    print(f\"   - Input field: {embedding_creator.text_field}\")\n",
    "    print(f\"   - Output field: {embedding_creator.embedding_field}\")\n",
    "    print(f\"   - Batch size: {embedding_creator.batch_size}\")\n",
    "    \n",
    "    # For this tutorial, let's use our normalized dataset (smaller for demonstration)\n",
    "    print(f\"\\nüîÑ Generating embeddings for {len(normalized_dataset.df)} documents...\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    start_time = time.time()\n",
    "    dataset_with_embeddings = embedding_creator(normalized_dataset)\n",
    "    embedding_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Embedding generation complete!\")\n",
    "    print(f\"‚è±Ô∏è  Time taken: {embedding_time:.2f} seconds\")\n",
    "    \n",
    "    # Inspect the results\n",
    "    embeddings_sample = dataset_with_embeddings.df.head(3).compute()\n",
    "    print(f\"\\nüìä Results preview:\")\n",
    "    print(f\"   - Total documents with embeddings: {len(dataset_with_embeddings.df)}\")\n",
    "    print(f\"   - Columns: {list(embeddings_sample.columns)}\")\n",
    "    \n",
    "    # Check embedding shape\n",
    "    if 'embeddings' in embeddings_sample.columns:\n",
    "        sample_embedding = embeddings_sample['embeddings'].iloc[0]\n",
    "        print(f\"   - Embedding shape: {np.array(sample_embedding).shape}\")\n",
    "        print(f\"   - Sample embedding values: {np.array(sample_embedding)[:5]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during embedding generation: {e}\")\n",
    "    print(\"üìù Note: EmbeddingCreator requires GPU setup. Using alternative approach...\")\n",
    "    \n",
    "    # Alternative: Generate embeddings manually for tutorial purposes\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    \n",
    "    # Get text data\n",
    "    text_data = normalized_dataset.df['text'].compute().tolist()\n",
    "    print(f\"üîÑ Generating embeddings for {len(text_data)} texts using fallback method...\")\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(text_data), EMBEDDING_BATCH_SIZE):\n",
    "        batch_texts = text_data[i:i+EMBEDDING_BATCH_SIZE]\n",
    "        batch_embeddings = model.encode(batch_texts)\n",
    "        all_embeddings.extend(batch_embeddings.tolist())\n",
    "    \n",
    "    # Add embeddings to dataset\n",
    "    df_with_embeddings = normalized_dataset.df.compute()\n",
    "    df_with_embeddings['embeddings'] = all_embeddings\n",
    "    dataset_with_embeddings = DocumentDataset.from_pandas(df_with_embeddings, backend=backend)\n",
    "    \n",
    "    print(f\"‚úÖ Fallback embedding generation complete!\")\n",
    "    print(f\"üìä Generated {len(all_embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b941ab",
   "metadata": {},
   "source": [
    "## 7. Vector Storage Setup and Configuration\n",
    "\n",
    "With our embeddings generated, we need to store them in a vector database for efficient similarity search and retrieval. We'll demonstrate multiple vector storage options including FAISS and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vector storage using FAISS\n",
    "print(\"üóÑÔ∏è  Setting up vector storage with FAISS...\")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    faiss_available = True\n",
    "    print(\"‚úÖ FAISS available\")\n",
    "except ImportError:\n",
    "    faiss_available = False\n",
    "    print(\"‚ö†Ô∏è  FAISS not available. Install with: pip install faiss-gpu (for GPU) or pip install faiss-cpu\")\n",
    "\n",
    "if faiss_available:\n",
    "    # Extract embeddings and metadata\n",
    "    df_with_emb = dataset_with_embeddings.df.compute()\n",
    "    embeddings_array = np.array(df_with_emb['embeddings'].tolist()).astype('float32')\n",
    "    \n",
    "    print(f\"üìä Preparing FAISS index:\")\n",
    "    print(f\"   - Number of vectors: {embeddings_array.shape[0]}\")\n",
    "    print(f\"   - Vector dimension: {embeddings_array.shape[1]}\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings_array.shape[1]\n",
    "    \n",
    "    # Use L2 distance for similarity (can also use IP for inner product)\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Optionally use GPU acceleration if available\n",
    "    if USE_GPU and hasattr(faiss, 'StandardGpuResources'):\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "            print(\"üöÄ Using GPU-accelerated FAISS index\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  GPU FAISS failed, using CPU index\")\n",
    "    \n",
    "    # Add vectors to index\n",
    "    print(\"üîÑ Adding vectors to FAISS index...\")\n",
    "    index.add(embeddings_array)\n",
    "    \n",
    "    print(f\"‚úÖ FAISS index created successfully!\")\n",
    "    print(f\"   - Index type: {type(index)}\")\n",
    "    print(f\"   - Total vectors: {index.ntotal}\")\n",
    "    print(f\"   - Is trained: {index.is_trained}\")\n",
    "    \n",
    "    # Save the index to disk\n",
    "    faiss_index_path = data_dir / \"faiss_index.bin\"\n",
    "    if hasattr(index, 'cpu_index'):  # GPU index\n",
    "        faiss.write_index(index.cpu_index, str(faiss_index_path))\n",
    "    else:  # CPU index\n",
    "        faiss.write_index(index, str(faiss_index_path))\n",
    "    \n",
    "    print(f\"üíæ FAISS index saved to: {faiss_index_path}\")\n",
    "    \n",
    "    # Save metadata mapping\n",
    "    metadata_mapping = df_with_emb[['doc_id', 'text', 'source', 'category']].reset_index(drop=True)\n",
    "    metadata_path = data_dir / \"metadata_mapping.parquet\"\n",
    "    metadata_mapping.to_parquet(metadata_path)\n",
    "    \n",
    "    print(f\"üíæ Metadata mapping saved to: {metadata_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping FAISS setup due to missing dependency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Set up ChromaDB vector database\n",
    "print(\"\\nüîÆ Setting up alternative vector storage with ChromaDB...\")\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    \n",
    "    # Create ChromaDB client\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        persist_directory=str(data_dir / \"chroma_db\"),\n",
    "        anonymized_telemetry=False\n",
    "    ))\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection_name = \"tutorial_embeddings\"\n",
    "    try:\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "        print(f\"‚úÖ Created new ChromaDB collection: {collection_name}\")\n",
    "    except:\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        print(f\"‚úÖ Using existing ChromaDB collection: {collection_name}\")\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    df_with_emb = dataset_with_embeddings.df.compute()\n",
    "    \n",
    "    # ChromaDB requires specific data format\n",
    "    documents = df_with_emb['text'].tolist()\n",
    "    embeddings = [emb.tolist() if isinstance(emb, np.ndarray) else emb for emb in df_with_emb['embeddings']]\n",
    "    ids = [str(i) for i in range(len(documents))]\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"doc_id\": row['doc_id'],\n",
    "            \"source\": row['source'],\n",
    "            \"category\": row['category']\n",
    "        }\n",
    "        for _, row in df_with_emb.iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(f\"üîÑ Adding {len(documents)} documents to ChromaDB...\")\n",
    "    \n",
    "    # Add to ChromaDB collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ChromaDB setup complete!\")\n",
    "    print(f\"   - Collection: {collection_name}\")\n",
    "    print(f\"   - Documents stored: {collection.count()}\")\n",
    "    print(f\"   - Persist directory: {data_dir / 'chroma_db'}\")\n",
    "    \n",
    "    chromadb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  ChromaDB not available. Install with: pip install chromadb\")\n",
    "    chromadb_available = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up ChromaDB: {e}\")\n",
    "    chromadb_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50dc26f",
   "metadata": {},
   "source": [
    "## 8. Indexing and Metadata Management\n",
    "\n",
    "Effective metadata management is crucial for retrieval systems. We need to efficiently store and retrieve both the vector embeddings and their associated metadata (document IDs, sources, categories, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
